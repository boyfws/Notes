\documentclass{article}

% Packages
\usepackage{amsmath}
\usepackage{enumitem} % Для настройки списков
\usepackage{amssymb} % Подключаем пакет для знака следствия
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{array}
\usepackage{tikzexternal}


\allowdisplaybreaks


\title{Классическое машинное обучение}
\author{Минкин Даниэль}

\begin{document}

    \maketitle

    \tableofcontents % Оглавление

    \section{Линейные модели}

    Линейные модели --- класс моделей которые используют линейное преобразование для вектора входных фичей.

    \subsection{Регрессия}

    Формализуем задачу регрессии, пусть у нас есть вектор $\bar{x} \in \operatorname{R}^{n}$.
    Тогда предсказание может быть сделано с помощью такой формулы:

    \begin{equation}
        y_{pred} = \bar{x} \cdot \bar{w} + w_{0}
    \end{equation}

    Т.е мы ищем такой вектор $\bar{w} \in \operatorname{R}^{n + 1}$, который будет выдавать наиболее близкие $y_{pred}$ к $y_{true}$.

    \subsection{Классификация}

    В случае решения задачи классификации через линейные модели сначала делается предсказание как в случае регрессии, а после
    к результату предсказания применяется разделяющее правило (например в случае бинарной классификации правило может задаваться так:
    если $y_{pred} > 0$ мы относим объект к положительному классу --- если нет, то к отрицательному)

    В случае бинарной классификации с разделяющим правилом из примера уравнение регрессии задает гиперплоскость,
    которая разделяет исходное пространство: i.e $\sum_{i = 1}^{n}{w_{i} \cdot x_{i}} + c > 0$ --- плоскость в n-мерном пространстве,
    которая делит пространство на положительный и отрицательные классы

    \subsection{Общее}

    Несколько фактов:

    \begin{itemize}
        \item \textbf{При использовании OneHot Encoding-а мы можем избавиться от одной encoded фичи}.
        Все просто: пусть у нас есть веса для каждой закодированной фичи $w_{1}, w_{2} ... w_{n}$, также у нас добавляется константа $c$
        к предсказанию по фичам.
        Давайте удалим последнюю фичу, тогда в случае если $x_{1} = 0 \ ... \ x_{n-1} = 0$ нам нужно добавить к константе еще и $w_{n}$, иначе результат изменится, следовательно
        константа в новой модели должна быть равна $w_{n} + c$. 
        Однако тогда нам нужно внести поправку в веса в случае если один из $x_{i} \ | \ i < n$ равен 1, чтобы
        результат остался таким же. 
        Мы можем просто вычесть из старых весов $w_{n}$, за счет того, что мы добавили его к const ничего не изменится.
        Таким образом мы успешно исключили одну encoded фичу, оставив результаты предсказаний без изменений. 
        \textbf{Важно заметить, что если мы работаем в модели без константы, то тогда данный подход не сработает}
        \item \textbf{Для более сложных зависимостей необходимо использовать новые фичи которые являются функциями от старых}.
        Т.е мы включаем в модель фичи задаваемые как $f(x_{1}, x_{2} ... x_{n})$
        \item Если между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл
  
    \end{itemize}
    
    \subsection{Оценка по МНК}

    \subsubsection{Общее}

    В первую очередь опустим свободный член, так как можно считать, что у нас просто есть доа признак, который всегда равен 1.
    Пусть функция потерь задается как Евклидова норма между предсказанными и истинными значениями.
    I.e мы решаем следующую задачу:

    \begin{equation}
        \Vert Xw - b \Vert_{euclidean} \rightarrow \min_{w}
    \end{equation}
    где $X$ --- матрица размера $(N, k)$, $N$ -- размер выборки, а $k$ --- кол-во фичей, т.е это матрица где в строки записаны вектора,
    по которым нужно сделать предсказания.

    Однако нам также нужно сделать поправку на размер выборки, чтобы значения функции потерь можно было сравнивать между собой для разных выборок.
    Получается задача выгладит так:

     \begin{equation}
        \frac{\Vert Xw - b \Vert_{euclidean}}{N} \rightarrow \min_{w}
     \end{equation}
    
    \textbf{Функция потерь является функционалом, так как принимает на вход три значения --- матрицу наблюдений, true значения предсказываемой перменной и функцию, которая возвращает некое значение по вектору наблюдений}

    Значение коэффициентов может быть получено через псевдообратную матрицу, по ее свойству:

    \begin{equation}
        \Vert X X^{+} b - b \Vert_{euclidean} \leq \Vert X w - b \Vert_{euclidean}
    \end{equation}
    для любого $w$

    \quad

    Так как деление на константу --- монотонное преобразование, данное решение минимизирует нашу функцию потерь

    \begin{quote}
        \textbf{Важно заметить:} псевдообратная матрица может быть использована и для других норм, однако тогда нам
        нужно перейти в евклидово пространство $A$ с новым скалярным произведением, тогда псевдообратная матрица будет минимизировать
        норму задаваемую как $\sqrt{\langle u, u \rangle_{A}}$.
        Об этом будет рассказано позже
    \end{quote}

    Таким образом решением данного СЛАУ будет

    \begin{equation}
        w_{*} = A^{+}b
    \end{equation}

    В случае если $N \geq k$
    
    \begin{equation}
        w_{*} = (X^{T} X)^{-1} X^{T} b
    \end{equation}

    А в противном случае, когда $N < k$

    \begin{equation}
        w_{*} = X^{T} (X X^{T})^{-1} b
    \end{equation}

    Мы можем считать, что $X$ --- матрица полного столбцового или строчного ранга, зачастую у нас не будет полностью ЛЗ столбцов или строк, а
    даже если они и есть их можно исключить из-за бессмысленности

    \subsubsection{Невырожденность матрицы $X^{T} X$}

    В реальных задачах матрицах $X^{T} X$ или $X X^{T}$ являются невырожденными, однако нам нужно оценить насколько они
    ``невырождены``, так как у нас есть погрешность при вычислении детерминанта, например мы могли получить неотрицательное значение
    из-за логики работы чисел с плавающей точкой или же в изначальных данных содержится погрешность.

    \quad

    \textbf{Число обусловленности}

    \quad


    Число обусловленности --- максимальное значение отношения относительного изменения функции и относительного изменениия арргмента

    \begin{equation}
        \mu(f, x) = \max_{\Delta x} \frac{ \frac{\Vert f(x + \Delta x) - f(x) \Vert}{\Vert f(x) \Vert}  }{ \frac{\Vert \Delta x \Vert}{\Vert x \Vert } }
    \end{equation}
    в малой окрестности $\Delta x$

    Легко заметить, что чем больше значение $\mu(f, x)$, тем хуже так как наше решение может очень сильно измениться от малого приращения аргумента, а мы бы хотели
    иметь ``стабильное`` решение

    \quad

    Покажем на зависимость числа обусловленности задачи $f(A) = (A^T A)^{-1}$ от матрицы корреляции.
    Мы воспользуемся определением числа обусловленности:

    \begin{equation}
        \mu(f, A) = \max_{\Delta A} \frac{ \frac{\Vert f(A + \Delta A) - (A^T A)^{-1}  \Vert }{\Vert (A^T A)^{-1} \Vert} }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} }
    \end{equation}

    Тут нам понадобится матричное дифференцирование, нам нужно найти дифференциал $(A^T A)^{-1}$, заметим, что это композиция двух функций.
    Во-первых, вспомним определение дифференциала, пусть задана функция $f: R^{n} \rightarrow R^{m}$
    Тогда ее дифференциал может быть найден так:

    \begin{equation}
        f(x_{0} + \Delta x) - f(x_{0}) = [D_{x_{0}} f](\Delta x) + o(\Vert \Delta x \Vert)
    \end{equation}

    При этом дифференциал $[D_{x_{0}} f]$ --- линейное отображение из $R^{n}$ в $R^{m}$, т.е мы находим линейную часть приращения отображения

    \quad

    Найдем дифференциал $f(X) = X^{T} X$ где $X$ - матрица $(n, m)$ и $n > m$.
    Тогда:
    \[
    \begin{gathered}
        f(X + \Delta X) - f(X) = (X + \Delta X)^{T} (X + \Delta X) - X^{T} X \\
        = (X^{T} + (\Delta X)^{T}) (X + \Delta X) - X^{T} X \\
        = X^{T} X + X^{T} \Delta X + (\Delta X)^{T} X + (\Delta X)^{T} \Delta X - X^{T} X \\
        = X^{T} \Delta X + (\Delta X)^{T} X + (\Delta X)^{T} \Delta X
    \end{gathered}
    \]
    Можно легко показать, что $[D_{X} f](\Delta X) = X^{T} \Delta X + (\Delta X)^{T} X$ является линейным оператором по $\Delta X$.
    При этом $(\Delta X)^{T} \Delta X$ и есть $o(\Vert \Delta X \Vert)$.

    \quad

    Найдем дифференциал $g(X) = X^{-1}$.
    Мы будем использовать равенство $E = X^{-1} X$.
    Продифференцируем выражение с обеих сторон.

    \begin{equation}
        0 = [D_{X_{0}} (X^{-1} X)](H)
    \end{equation}

    По правилам дифференцирования умножения мы получаем

    \begin{equation}
        0 = [D_{X_{0}} (X^{-1})](H) \cdot X_{0} + X_{0}^{-1} \cdot [D_{X_{0}} (X)](H)
    \end{equation}

    Таким образом

    \begin{equation}
        [D_{X_{0}} (X^{-1})](H) \cdot X_{0} = - X_{0}^{-1} \cdot [D_{X_{0}} (X)](H)
    \end{equation}

    Т.е все сводится к формуле

    \begin{equation}
        [D_{X_{0}} (X^{-1})](H) = - X_{0}^{-1} \cdot [D_{X_{0}} (X)](H) \cdot X_{0}^{-1}
    \end{equation}

    При этом $[D_{X_{0}} (X)](H) = H$, таким образом итоговая формула имеет вид

    \begin{equation}
        [D_{X_{0}} (X^{-1})](H) = - X_{0}^{-1} \cdot H \cdot X_{0}^{-1}
    \end{equation}

    Мы имеем дело с функцией $g(f(X))$, дифференциал такой функции представим как

    \begin{equation}
        [D_{f(X_{0})} (g)]([D_{X_{0}} (f)] (\Delta X))
    \end{equation}

    Следовательно $[D_{X_{0}} ((X^{T} X)^{-1})](\Delta X)$, может быть найдено так

    \begin{equation}
       (X_{0}^{T} X_{0})^{-1} \cdot (X_{0}^{T} \Delta X + (\Delta X)^{T} X_{0}) \cdot (X_{0}^{T} X_{0})^{-1}
    \end{equation}

    Таким образом в изначальной формуле числа обусловленности мы можем записать приращение функции как

    \begin{equation}
        \mu(A) = \max_{\Delta A} \frac{ \frac{\Vert  (A^{T} A)^{-1} \cdot (A^{T} \Delta A + (\Delta A)^{T} A) \cdot (A^{T} A)^{-1} \Vert }{\Vert (A^T A)^{-1} \Vert} }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} }
    \end{equation}

    Оценим наше выражение сверху, сделаем несколько предположений на будущее, \textbf{пусть мы используем одно и тоже семейство норм для входного и выходного пространства и для этого семейства выполняется свойство субмультипликативности},
    тогда для $(A^{T} A)^{-1}$ и $(A^{T} \Delta A + (\Delta A)^{T} A)$ можно будет использовать следующее мажорирование

    \begin{equation}
        \Vert  (A^{T} A)^{-1}  (A^{T} \Delta A + (\Delta A)^{T} A) (A^{T} A)^{-1} \Vert  \leq \Vert (A^{T} A)^{-1} \Vert^{2} \Vert A^{T} \Delta A + (\Delta A)^{T} A \Vert
    \end{equation}

    В итоге мы можем мажорировать наше выражение так:

    \[
    \begin{gathered}
        \frac{ \frac{\Vert  (A^{T} A)^{-1} \cdot (A^{T} \Delta A + (\Delta A)^{T} A) \cdot (A^{T} A)^{-1} \Vert }{\Vert (A^T A)^{-1} \Vert} }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} } \leq \\
        \frac{ \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A^{T} \Delta A + (\Delta A)^{T} A \Vert }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} } \\
    \end{gathered}
    \]

    Продолжая мажорирование мы получим следующую ситуацию

    \[
    \begin{gathered}
        \frac{ \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A^{T} \Delta A + (\Delta A)^{T} A \Vert }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} } \leq \\
        \frac{ \Vert (A^{T} A)^{-1} \Vert \cdot ( \Vert A^{T} \Delta A \Vert + \Vert (\Delta A)^{T} A \Vert) }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} }
    \end{gathered}
    \]

    Предположим, что норма выходного пространства устойчива к транспонированию, также вспомним условие про субмультипликативность, тогда наше выражение примет вид

    \[
        \begin{gathered}
            \frac{ \Vert (A^{T} A)^{-1} \Vert \cdot ( \Vert A^{T} \Delta A \Vert + \Vert (\Delta A)^{T} A \Vert) }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} } = \\
            2 \cdot \frac{ \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A^{T} \Delta A \Vert  }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} } \leq \\
            2 \cdot \frac{ \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A^{T} \Vert \cdot \Vert \Delta A \Vert  }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} } = \\
            2 \cdot \frac{ \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A^{T} \Vert  }{ \frac{1}{\Vert A \Vert} } = \\
            2 \cdot \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A \Vert^{2}
        \end{gathered}
    \]

    Таким образом, при определенных требованиях к семейству норм мы получаем, что

    \[
        \begin{gathered}
            \max_{\Delta A} \frac{ \frac{\Vert  (A^{T} A)^{-1} \cdot (A^{T} \Delta A + (\Delta A)^{T} A) \cdot (A^{T} A)^{-1} \Vert }{\Vert (A^T A)^{-1} \Vert} }{ \frac{\Vert \Delta A \Vert}{\Vert A \Vert} } \leq \\
            2 \cdot \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A \Vert^{2}
        \end{gathered}
    \]

    Вместо точного максимума мы можем использовать оценку сверху для всех значений.

    Покажем влияние на данный результат близости матрицы к ``вырожденной``.
    Применим сингулярное разложение к матрице $A$, если она имеет размеры $(n, m)$, где $n > m$

    \begin{equation}
        A = U_{(n, n)} \Sigma_{(n, m)} V^{T}_{(m, m)}
    \end{equation}

    $\Sigma$ --- диагональная матрица где на диагонали находятся сингулярные числа матрицы, а $U$ и $V$ --- унитарные матрицы, тогда
    $A^{T}A$ равно

    $V \Sigma^{T} U^{T}  U \Sigma V^{T} = V \Sigma^{T} E \Sigma V^{T}$.
    Тогда:

    \begin{equation}
    (A^{T}A)^{-1} = (V \Sigma^{T} E \Sigma V^{T})^{-1}
    \end{equation}

    При этом $\Sigma^{T} E \Sigma $ --- матрица $(m, m)$, если матрица $A$ имеет размер $(n, m)$, где $n > m$.
    Так как матрица $A$ имеет $m$ сингулярных значений и является матрицей полного столбцового ранга ---
    матрица $\Sigma^{T} \Sigma$ является обратимой.
    Продолжим раскрывать наше выражение

    \begin{equation}
        (A^{T}A)^{-1} = V (\Sigma^{T} \Sigma)^{-1} V^{T}
    \end{equation}

    Мажорируем наше старое выражение, оно будет равно

    \begin{equation}
        2 \cdot \Vert (A^{T} A)^{-1} \Vert \cdot \Vert A \Vert^{2} \leq 2 \cdot \Vert (\Sigma^{T} \Sigma)^{-1} \Vert \cdot \Vert V \Vert^{2} \cdot \Vert A \Vert^{2}
    \end{equation}

    $\Sigma^{T} \Sigma$ --- квадратная матрица на диагонали у которой находятся квадраты сингулярных значений матрицы,
    следовательно, обратная матрица к ней будет иметь вид $\operatorname{diag}(\frac{1}{\sigma_{1}^{2} }, \ \frac{1}{\sigma_{2}^{2} } \ ... \ \frac{1}{\sigma_{m}^{2} })$


    \begin{equation}
        2 \cdot \Vert (\Sigma^{T} \Sigma)^{-1} \Vert \cdot \Vert V \Vert^{2} \cdot \Vert A \Vert^{2} = 2 \cdot \Vert     \operatorname{diag}(\frac{1}{\sigma_{1}^{2} }, \ \frac{1}{\sigma_{2}^{2} } \ ... \ \frac{1}{\sigma_{m}^{2} })\Vert \cdot \Vert V \Vert^{2} \cdot \Vert A \Vert^{2}
    \end{equation}

    При этом сингулярные числа показывают близость матрицы к невырожденной, или же близость к матрице полного столбцового/строкового ранга,
    если матрица $A$ имеет сильную зависимость между столбцами то $\min\{\sigma_{1}, \sigma_{2} ... \} \rightarrow 0$, что ведет к увеличению числа обусловленности (сингулярные числа рассмотрю позже)

    \quad

    \textbf{Второй подход к числу обусловленности}

    \quad

    Мы можем сделать проще и рассмотреть число обусловленности матрицы $A$ как линейного оператора.
    Рассмотрим уравнение:

    \begin{equation}
        Ax = b
    \end{equation}

    Тогда решением в общем виде является

    \begin{equation}
        x = A^{+} b
    \end{equation}

    Рассмотрим число обусловленности

    \begin{equation}
        \mu(A, b) = \max_{\Delta b} \frac{ \frac{\Vert A^{+} (b + \Delta b) - A^{+} b \Vert}{ \Vert A^{+} b \Vert } }{ \frac{\Vert b + \Delta b - b \Vert}{\Vert b \Vert} } =
        \max_{\Delta b} \frac{ \frac{\Vert A^{+} \Delta b \Vert}{ \Vert A^{+} b \Vert } }{ \frac{\Vert \Delta b \Vert}{\Vert b \Vert} }
    \end{equation}

    Перенеся деление

    \begin{equation}
        \max_{\Delta b} \frac{ \Vert A^{+} \Delta b \Vert \cdot  \Vert b \Vert   }{ \Vert \Delta b \Vert \cdot  \Vert A^{+} b \Vert } =
        \max_{\Delta b} (\frac{\Vert A^{+} \Delta b \Vert}{\Vert \Delta b \Vert}) \cdot (\frac{\Vert b \Vert }{\Vert A^{+} b \Vert})
    \end{equation}

    \textbf{Аналогично прошлому случаю затребуем свойство субмультипликативности от нормы}, тогда мы можем мажорировать первый множитель
    с помощью $\Vert A^{+} \Vert$.
    Следовательно:

    \begin{equation}
        \mu(A, b) = \Vert A^{+} \Vert \frac{\Vert b \Vert }{\Vert A^{+} b \Vert} = \Vert A^{+} \Vert \frac{\Vert Ax + \epsilon \Vert }{\Vert x \Vert}
    \end{equation}
    где $\epsilon$ - вектор отклонений

    \begin{equation}
        \Vert A^{+} \Vert \frac{\Vert Ax + \epsilon \Vert }{\Vert x \Vert} \leq \Vert A^{+} \Vert \frac{\Vert Ax \Vert + \Vert \epsilon \Vert }{\Vert x \Vert}
    \end{equation}

    Таким образом мы получаем

    \begin{equation}
        \Vert A^{+} \Vert \frac{\Vert Ax \Vert} {\Vert x \Vert}  + \Vert A^{+} \Vert \frac{ \Vert \epsilon \Vert }{\Vert x \Vert} \leq \Vert A^{+} \Vert \frac{\Vert A \Vert x \Vert} {\Vert x \Vert}  + \Vert A^{+} \Vert \frac{ \Vert \epsilon \Vert }{\Vert x \Vert}
    \end{equation}

    Финальное выражение выглядит так:

    \begin{equation}
        \Vert A^{+} \Vert \Vert A \Vert   + \Vert A^{+} \Vert \frac{ \Vert \epsilon \Vert }{\Vert x \Vert}
    \end{equation}

    В случае если используется сингулярная норма: Сингулярная норма матрицы --- ее максимальное сингулярное значение,
    а псевдообратная матрица имеет обратные сингулярные значения к исходной матрице (легко показать),
    таким образом:
    \begin{equation}
        \Vert A \Vert_{spec} = \sigma_{max}(A)
    \end{equation}

    и

    \begin{equation}
        \Vert A^{+} \Vert_{spec} = \frac{1}{\sigma_{min}(A)}
    \end{equation}

    Таким образом мы получаем:

    \begin{equation}
        \mu(A, b) = \frac{\sigma_{max}(A)}{\sigma_{min}(A)} + \Vert A^{+} \Vert \frac{ \Vert \epsilon \Vert }{\Vert x \Vert}
    \end{equation}

    Если пренебречь вторым слагаемым, то мы получим то, что описано в учебнике ШАДА: 
    ``Пожертвовав математической строгостью, мы можем считать, что число обусловленности матрицы $X$ – 
    это корень из отношения наибольшего и наименьшего из собственных чисел матрицы $X^{T}X$``.
    \quad

    \textbf{Пояснение:}
    Сингулярные значения матрицы $A$ --- это квадратные корни из собственных значений матрицы $A^{T}A$

    \quad

    \textbf{Intuition проблемы числа обусловленности}

    \quad

    Матрица $A$ --- линейное отображение из одного пространства в другое, если в новом пространстве есть вектора с сильной линейной связью, то
    оно становится более сжатым, т.е непохожие вектора в исходном пространстве могут стать сильно более похожими в новом пространстве за счет того,
    что оно сжато в размерах, таким образом если мы немного изменим целевой вектор (из нового пространства), то вектор который его образовал может сильно отличаться от того, что мы
    получили в прошлый раз, так как за счет сжатия они лежат рядом в новом пространстве, однако не в старом, таким образом: число обусловленности ---
    лишь следствие того, что новое пространство сжато

    Для наглядности можно рассмотреть, двумерный случай,
    рассмотрим матрицу перехода в новое пространство

    \[
    \begin{pmatrix}
    1 & 1 \\

    1 & 1 + \epsilon
    \end{pmatrix}
    \]
    где $\epsilon > 0$

    Мы видим, что $ \left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right] $ отобразится в $ \left[ \begin{smallmatrix} 1 \\ 1 \end{smallmatrix} \right] $, а
    $ \left[ \begin{smallmatrix} 0 \\ 1 \end{smallmatrix} \right] $ в $ \left[ \begin{smallmatrix} 1 \\ 1 + \epsilon \end{smallmatrix} \right] $.
    При маленьких $\epsilon$ матрица остается невырожденной, однако пространство ``сжимается`` делая $ \left[ \begin{smallmatrix} 1 \\ 0 \end{smallmatrix} \right] $ и
    $ \left[ \begin{smallmatrix} 0 \\ 1 \end{smallmatrix} \right] $ все менее отличимыми в новом пространстве





    \subsubsection{Переход к новому скалярному произвдениию}

    Мы хотим перейти в пространство с новым скалярным произведением, однако большинство компьютерных систем заточены
    под обычное Евклидово скалярное произведение, поэтому нам бы хотелось перейти в такой базис, чтобы операции в этом базисе
    с использованием ``обычного`` скалярного произведения были равносильны операциям с использованием ``нового`` скалярного произведения
    в исходном пространстве

    Заметим, что любое произвольное скалярное произведение может быть задано как квадратичная форма, исходя из свойства линейности.

    \begin{equation}
        \langle u, v \rangle_{G} = u^{T} G v
    \end{equation}

    Заметим, что $G$ --- положительно определена и симметрична, по свойству скалярного произведения.
    Мы можем найти матрицу перехода $P$ таким образом, через разложение Холецкого.

    \begin{equation}
        G = P^{T} P
    \end{equation}

    Тогда вектор $k$ и матрица $M$ в новом пространстве превращаются в $Pk$ и $P M P^{T}$ соответственно

    \quad

    \textbf{Пример}

    \quad

    Пусть мы работаем в новом пространстве и хотим умножить матрицу на вектор в новом пространстве.
    У нас есть $k' = Pk$ и $M' = P M P^{T}$, мы хотим, чтобы итоговый вектор имел вид:

    \[
    Mk = \begin{bmatrix}
    \langle M_{1, *}, k \rangle_{G} \\
    \langle M_{2, *}, k \rangle_{G} \\
    \vdots \\
    \langle M_{n, *}, k \rangle_{G}
    \end{bmatrix}
    \]

    это равносильно:


    \[
    Mk = \begin{bmatrix}
    M_{1, *}^{T} G k  \\
    M_{2, *}^{T} G k  \\
    \vdots \\
    M_{3, *}^{T} G k
    \end{bmatrix}
    \]

    следовательно

    \begin{equation}
        [Mk]_{new \ dot \ product} = MGk
    \end{equation}

    Рассмотрим линейный оператор $MG$, тогда в новом базисе данный оператор может быть представлен как $P MG P^{-1}$

    \begin{equation}
        P MG P^{-1} = P M P^{T} P P^{-1} = P M P^{T}
    \end{equation}

    Проверим, что это работает

    \begin{equation}
        P^{-1} (P M P^{T} Pk) = P^{-1} (P M G k) = MGk
    \end{equation}

    Таким образом

    \begin{equation}
        P^{-1} M' k' = MGk
    \end{equation}

    Покажем, что это сработает и для умножения на вектор справа

    \begin{equation}
         k'^{T} M' {P^{-1}}^{T} =  k^{T} P^{T} P M P^{T} {P^{-1}}^{T} = k^{T} G M
    \end{equation}

    Таким образом мы выполнили поставленную перед нами задачу

    \subsection{Сложность точного решения}

    Алгоритмическая сложность точного решения для матрицы $A$ размерами $(N, D)$ задается как:

    \begin{equation}
        O(D^2N + D^3 + DN + D^{2})
    \end{equation}

    Из них мы тратим $D^{2}N$ на перемножение матриц $A^{T}$ и $A$, а $D^{3}$ --- на обращение данной матрицы,
    $DN$ --- на умножение $A^{T}$ на $b$ и также тратим $D^{2}$ на умножение обратной матрицы на вектор $A^{T}b$

    \quad

    \textbf{Рассмотрим способы ускорения данных расчетов}

    \quad

    \begin{itemize}
        \item Во-первых легко заметить, что обращение матрицы имеет кубическую сложность, что плохо для задач с большим количеством фичей, рассмотрим
        итерационный алгоритм Шульца.
        Тогда $X_{k+1} = 2 X_{k} - X_{k} A X_{k} = X_{k} (2E - A X_{k})$, где $A$ --- исходная матрица, нужно заметить, что умножение $A$ на $X_{k}$ требует
        $O(D^{3})$ итераций, однако на практике это не равносильно такому же количеству итераций, как и при обращении матрицы за счет возможности использования
        векторизации и распараллеливания расчетов, после нам требуется еще $D^{2}$ операций на вычитание и еще $D^{3}$ на перемножение итоговой матрицы с $X_{k}$.
        Также доп итерации тратятся на проверку сходимости, которая проводится через такую норму $\Vert X_{k}A - E \Vert$.
        Таким образом, при долгой сходимости данный способ может проигрывать точным алгоритмам по итеряциям
        \item Для симметричных матриц хорошо применимы методы Крылова, такие как Conjugate Gradient,
        MINRES, SYMMLQ, которые дают квадратичную сложность (слишком сложная тема, чтобы тут подробно раскрывать)
        \item Умножение матриц может быть распараллелено, например на GPU
    \end{itemize}


    \subsection{Использование разложений для решения задачи}

    \subsubsection{Построим $QR$ разложение матрицы $A$}

    В данном разложении столбцы матрицы $Q$ отнормированны и имеют единичную длину, i.e $Q^{T}Q = E$, а $R$ --- верхнетреугольная квадратная матрица, тогда:

    \begin{equation}
        w = (R^{T}Q^{T}QR)^{-1} R^{T}Q^{T} b = (R^{T}R)^{-1} R^{T}Q^{T} b = R^{-1} {R^{T}}^{-1} R^{T}Q^{T} b
    \end{equation}

    Таким образом итоговая формула будет иметь вид

    \begin{equation}
        w = R^{-1} Q^{T} b
    \end{equation}

    Для матрицы $A$ размером $(N, D)$ \ $QR$ разложение будет иметь сложность сложность $O(ND^{2} - \frac{D^{3}}{3})$,
    также мы затратим $O(D^{3})$ на обращение $R$ (мы можем сократить кол-во итераций за счет того, что $R$ --- верхнетреугольная матрица).
    Плюсами такого решения является то, что мы снижаем кол-во операций перемножения матриц, что повышает численную стабильность
    и снижает общее кол-во итераций.
    Суммарное кол-во итераций задается так:

    \begin{equation}
        O(ND^{2} + D^{3}  \frac{2}{3} + DN + D^{2})
    \end{equation}

    Мы получаем $O(DN)$ за счет перемножения $Q^{T} b$ и $O(D^{2})$ за счет финального перемножения вектора $R^{-1}$ и $Q^{T} b$.
    В итоге сложность примерно такая же как и в прошлом случае, однако часть операций структурно отличается от него, что может позволить ускорить
    алгоритм.


    \subsubsection{Построим сингулярное разложение матрицы $A$}


    Про сингулярное разложение уже было сказано выше.
    Однако в данном случае мы будем использовать усеченное сингулярное разложение, где матрица с сингулярными значениями
    является квадратной, а $U$ и $V$ --- ортогональны по столбцам (i.e $U^{T}U = E$ и $V^{T}V = E$) тогда $A = U \Sigma V^{T} $.
    Тогда:

    \begin{equation}
        w = (V \Sigma U^{T} U \Sigma V^{T})^{-1} V \Sigma U^{T} b = (V \Sigma \Sigma V^{T})^{-1} V \Sigma U^{T} b
    \end{equation}

    Раскроем обратную матрицу

    \begin{equation}
        w = {V^{T}}^{-1} \Sigma^{-1} \Sigma^{-1} V^{-1} V \Sigma U^{T} b = {V^{T}}^{-1} \Sigma^{-1}  U^{T} b = V \Sigma^{-1}  U^{T} b
    \end{equation}

    Данное решение хорошо себя ведет в случае плохой обусловленности матрицы $A$








\end{document}