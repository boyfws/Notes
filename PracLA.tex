
\documentclass{article}

% Packages
\usepackage{amsmath}
\usepackage{enumitem} % Для настройки списков
\usepackage{amssymb} % Подключаем пакет для знака следствия
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{array}
\usepackage{tikzexternal}
\usepackage{translit}


\allowdisplaybreaks


\title{Практическая линейная алгебра}
\author{Минкин Даниэль}

% Document
\begin{document}
    \maketitle

    \tableofcontents % Оглавление

    \section{Введение}

    \subsection{Темы}

    Вот ключевые темы курса:

    \begin{itemize}
        \item Псевдообратная матрица
        \item Линейная регрессия и МНК
        \item Полиномиальная интерполяция, сплайны, кривые Безье (возможно сплайны Безье)
        \item Аппроксимация (лишь часть связанная с лин. алгеброй) (Многочлены Чебышева)
        \item Метрики и нормы
        \item Оценка погрешности стандартных задач линейной алгебры
        \item Итерационные алгоритмы
        \item Проблемы собственных значений и собственных векторов
        \item Неотрицательные и положительные матрицы и их специальные свойства (Теорема Фробениуса — Перрона). Пример использования данной теоремы --- RageRank
        \item Функции от матриц
        \item (Если успеем) Системы алгебраических уравнений
        \item (Если успеем) Линейные экономические модели
    \end{itemize}

    \quad

    В процессе также будут затронуты:

    \quad

    \begin{itemize}
        \item Разложения матриц
        \item Приближение малого ранга (a.k.a PCA в общем виде)
    \end{itemize}

    \subsection{Организация курса}

    Будет 2 онлайн кр (как на выч. стате дают сложные задачи на подумать) в конце каждого из модулей. 
    Формула оценки задается как: 
    
    \begin{equation}
       0.5 \cdot \text{КР1} + 0.5 \cdot \text{КР2} + \text{Бонус}
    \end{equation}

    Бонус будет формироваться в зависимсоти от активности на парах, НО основная его часть будет получена за формирование доклада по линейной алгебре, который будет презентован на одной из пар.

    \textbf{Чел упомянул интересную тему для доклада: существует теорема, по которой мы с помощью двух линейных слоев и функции активации можно аппроксимировать любую функцию с заданной точностью}


    \section{Разложение полного ранга (скелетное разложение)}

    Пусть $A$ матрица $(m, n)$, такая что $\operatorname{rang}(A) = r$.
    Разложение $A = FG$, где $F$ имеет размерность $(m, r)$, а $G$ имеет размерность $(r, n)$ называется
    разложением полного ранга.

    Заметим, что $\operatorname{rang}(F) <= r$ из-за размера, при этом $\operatorname{rang}(F) >= r$ по свойству умножения матриц.
    Следовательно $\operatorname{rang}(F) = r$, это матрица полного столбцового ранга, аналогично получаем, что $G$ матрица полного строкового ранга.
    Таким образом матрица $A$ представима, как произведение двух полнорангвых матриц

    \subsection{Теорема о существовани скелетеного разложения}

    Для любой матрицы существуют скелетное разложение

    \quad

    \textbf{Доказательство}

    \quad

    Покажем, что для любой матрицы существует скелетное разложение,
    у нас есть $r$ независимых столбцов внутри матрицы.
    Выберем первые $r$ столбцов (мы можем так сделать так, как нам не важно какие брать, остальные точно могут быть выражены через них ),
    тогда матрица $A$ представима как
    $A = (\bar{a}_{1} \ ... \ \bar{a}_{r} \ \sum^{r}_{i = 1}{w_{1, i}  \bar{a}_{i}} \ ... \  \sum^{r}_{i = 1}{w_{n - r, i} \bar{a}_{i}})$.
    Пусть тогда матрица $F$ равна $(\bar{a}_{1} \ ... \ \bar{a}_{r})$, она имеет размер $(m, r)$.
    По условию отбора данных столбцов они все линейно независимы, таким образом $\operatorname{rang}(F) = r$, так как
    $\operatorname{rang}(F) <= r$ (по условию размера матрицы) и при этом $\operatorname{rang}(F) >= r$ (так как все столбцы ЛНЗ).

    Рассмотрим матрицу $G$, которая выглядит так $G = (e(1) \ ... \ e(r) \ w_{1} \ ... \ w_{n - r})$, где
    $e(i)$ --- вектор с размерностью $m$ где все координаты равны $0$, а $i$ - ая координата равна 1.
    Такая матрица имеет размерность $(r, n)$ и ее первые $r$ столбцов линейно независимы, а остальные выражаются через них, следовательно
    $\operatorname{rang}(G) = r$.
    При этом $FG = A$.
    Ч.Т.Д

    \subsection{Практический способ нахождения склетного разложения}

    Рассмотрим практический способ нахождения данного разложения.
    Приведем матрицу $A$ с размерами $(m, n)$ к каноническому виду (на диагонали 1, а под ними ничего).
    У нас получится $r$ ступенек, вырежем верхний левый блок канонической матрицы, размером
    $(r, n)$.
    Далее выберем столбцы у которых в каноническом виде в столбце всего лишь одна $1$ и во всех остальных местах нули.
    Добавим эти столбцы в новую матрицу, тогда это будет матрица $F$





    \section{Псевдообратная матрица}
    
    \textbf{Почему мы не можем всегда использовать метод Гаусса для решения СЛАУ?}

    \quad

    Самый большой минус - сложность задаваемая как $O(n ^ {2})$ для квадратной матрицы $n$ на $n$.

    Вторая проблема - аккумуляция погрешности при использовании данного метода, пример: при вычитании первой строчки из второй, ошибка первой строчки суммируется со второй.
    Плюс также погрешность создают float-ы (особенно с первой причиной вместе)

    \quad 

    Вспомним, что такое обратная матрица.
    Это матрица $A^{-1}$ такая, что

    \begin{equation}
        A A^{-1} = A^{-1} A = \operatorname{E}
    \end{equation}

    Пусть матрица $A$ имеет размеры $m$ на $n$, попробуем проверить условия, которыми задаются условия для обратной матрицы

    Тогда $A A^{-1} = \operatorname{E}$, тут $\operatorname{E}$ будет иметь размеры $m$ на $m$, а $A^{-1}$ иметь размеры $n$ на $m$.
    В обратном же случае $A^{-1} A = \operatorname{E}$ матрица $\operatorname{E}$ будет иметь размеры $n$ на $n$.

    Если мы будем использовать лишь одно из условий, тогда очень легко определить обратную матрицу

    \subsection{Немного о ранге}

    $\operatorname{rang}$ может быть определен как

    \begin{itemize}
        \item Наибольшее кол-во линейно независимых строк/столбцов в матрице
        \item $\dim(\operatorname{Im}(A) )$, где $A$ --- наша матрица, а $\operatorname{Im}$ --- образ нашей матрицы.
    \end{itemize}

    \quad

    \textbf{Вспоминаем условие про ранг суммы и произведения}

    \quad

    \textbf{Ранг суммы}

    Рассмотрим выражение $\operatorname{rang}(A + B)$ и попробуем его оценить.
    Очевидно, что $\operatorname{rang}(A + B) <= \operatorname{rang}(A) + \operatorname{rang}(B)$.
    Почему так?
    По свойству линейных операторов можно заметить, что $(A + B)v = Av + Bv$, следовательно,
    любой вектор из $\operatorname{Im}(A + B)$ является подпространством $\operatorname{Im}(A) + \operatorname{Im}(B)$.
    При этом $\operatorname{Im}(A) + \operatorname{Im}(B) = \{u + v | u \in \operatorname{Im}(A), v \in \operatorname{Im}(B)\}$.
    По свойству пространств $\dim(U + V) = \dim(U) + \dim(V) - \dim(U \cap V)$ (Это уже на 100\% интуитивно).
    Следовательно $\dim(U + V) <= \dim(U) + \dim(V)$.
    Применяя данное уравнение к образам, получим $\dim(\operatorname{Im}(A + B)) <= \dim(\operatorname{Im}(A)) + \dim(\operatorname{Im}(B))$,
    что исходя из определения $\operatorname{rang}$ приводит к $\operatorname{rang}(A + B) <= \operatorname{rang}(A) + \operatorname{rang}(B)$.
    Ч.Т.Д

    \quad 
    
    \textbf{Ранг произведения}

    $\operatorname{rang}(AB) <= \max(A, B)$.
    В данном случае посмотрим на суть произведения матриц --- это композиция линейных операторов.
    Т.е функция от функции, сначала применяется линейный оператор $B$, а к результату применяется оператор $A$.
    Пусть $A$ --- матрица $(m, n)$, а $B$ --- матрица $(n, k)$.
    Рассмотрим процесс работы данной композиции, к нам приходит вектор с размерностью $k$, его отображают в пространство с размерностью
    $\operatorname{rang}(B)$, а после к нему применяется оператор $A$, заметим,
    что он не может отобразить вектор в пространство с размерностью больше чем $\operatorname{rang}(B) \Rightarrow
    \operatorname{rang}(AB) <= \operatorname{rang}(B)$, при этом выходная размерность также не может превысить $\operatorname{rang}(A)$.
    Таким образом $\operatorname{rang}(AB) <= \min(\operatorname{rang}(B), \operatorname{rang}(A)) <= \max(\operatorname{rang}(B), \operatorname{rang}(A))$

    \subsection{Снова возврат к псевдообратной матрице}

    \begin{equation}
        \operatorname{rang}(A A^{-1}) = m
    \end{equation}

    \begin{equation}
        \operatorname{rang}(A^{-1} A) = n
    \end{equation}

    При этом $\operatorname{rang}(A) <= \min(m, n)$.
    Исходя из неравенств выше получаем,
    что $\operatorname{rang}(A) >= m$ и $\operatorname{rang}(A) >= n$
    (Так как максимальный элемент из $\operatorname{rang}(A)$ и $\operatorname{rang}(A^{-1})$
    больше данных значений).
    \textbf{Однако в таком случае возникает противоречие, если $m \neq n$}.
    Это говорит о том, что мы не можем использовать такие аксиомы для нашей ``обратной``
    матрицы, которая может быть применена в общем случае


    \subsection{Рассмотрим виды СЛАУ}

    Какие виды систем уравнений бывают:

    \begin{itemize}
        \item \textbf{Определенная} --- у нас есть матрица $(m, n)$, где $m > n$.
        Система имеет единственное решение.
        Мы хотим представить решение как $J b$,
        когда уравнение задается как $Ax = b$, где $J$ --- произвольная матрица с размерами
        $(n, m)$.
        \item \textbf{Не определенная} --- решений бесконечно много.
        У нас возникает вопрос как выразить хоть какое то решение
        \item \textbf{Не совместна} --- решений нет.
        Мы хотим получить наиболее правдоподобное приближение к решению
    \end{itemize}

    \subsection{Эрмитова матрица}

    Введем операцию комплексного (эрмитова) сопряжения для матрицы.
    Пусть дана матрица $A$, тогда эрмитово сопряженная матрица (которая обозначается как $A^{*}$)
    это транспонированная матрица $A$ в которой каждый элемент заменили на комплексно сопряженный ему.
    Т.е для любого $a \in \mathbb{C} \backslash \mathbb{R}$ в матрице $A$ $\overline{a + b i} = a - bi$.
    Новая матрица называется \textbf{эрмитово сопряженной}

    \quad

    \textbf{Эрмитова матрица} --- квадратная матрица, которая при транспонировании равна совей эрмитово сопряженной матрице
    \begin{equation}
        A^{T} = A^{*}
    \end{equation}

    \quad

    \textbf{NOTE:} Любая симметричная матрица является эрмитовой

    \subsection{Наконец то опредлеим псевдообратную матрицу}

    Пусть $A$ --- некая прямоугольная матрица.
    Матрица $C$ называется псевдообратной матрицей (а точнее псевдообратной Мура-Пенроуза),
    если выполняются следующие 4 аксиомы Пенроуза

    \begin{itemize}
        \item $ACA = A$
        \item $CAC = C$
        \item $(AC)^{*} = AC$ (i.e это эрмитова матрица)
        \item $(CA)^{*} = CA$ (тоже эрмитова)
    \end{itemize}

    \quad

    Важные замечания которые следуют из аксиом

    \begin{itemize}
        \item Если $A$ --- матрица $(m, n)$, то $C$ имеет размер $(n, m)$
        \item Если $A$ --- невырожденная квадрантная матрица, то псевдообратная матрица совпадет с обратной матрицей
    \end{itemize}

    \quad

    \textbf{Покажем что данная матрица единственна}

    \quad

    1. Пусть \(B\) и \(C\) — две псевдообратные матрицы для \(A\).
    2. Тогда:
       \[
       B = BAB = B(AC A)B = (BA)(CA)B.
       \]
    3. Поскольку \(BA\) и \(CA\) эрмитовы, их можно переставлять в сопряжённом виде:

        \begin{align*}
        BACAB &= (BA)^* (CA)^* B
              = A^* B^* A^* C^* B
              = (ABA)^* C^* B \\
              &= A^* C^* B
              = (CA)^* BAB
              = CABAB \\
              &= CAB.
        \end{align*}

    4. Аналогично:
       \[
       C = CAC = C(AB A)C = (CA)(BA)C = C A B.
       \]
    5. Из \(B = CAB\) и \(C = CAB\) следует \(B = C\), что доказывает единственность.

    \quad

    \textbf{Введем специальный символ}

    \quad

    Псевдообратную матрицу принято обозначать, как $A^{+}$

    \quad

    \subsection{Немного опредлений}

    \begin{quote}
        Матрица $A$ $(m, n)$ называется \textbf{матрицей полного столбового ранга}, если $\operatorname{rang}(A) = n$.
        При этом $m > n$.
        Т.е все ее столбцы Л.Н.З
    \end{quote}

    \quad

    \begin{quote}
        Матрица $A$ $(m, n)$ называется \textbf{матрицей полного строкового ранга}, если $\operatorname{rang}(A) = m$.
        При этом $m < n$.
        Т.е все ее строки Л.Н.З
    \end{quote}

    \quad

    Вставил не по порядку рассказа чисто по своему желанию:

    \quad

    \begin{quote}
        Вспомним определение ядра: $\ker(A) = \{v | Av = \bar{0} \} $.
        При этом по свойству ядра $\dim(\operatorname{Im}(A)) + \dim(\ker(A)) = n$, для матрицы $(m, n)$
    \end{quote}

    \subsection{Примеры работы}

    \subsubsection{Частный случай}

    Рассмотрим матрицу $(1 \ 1)$.
    Нам нужно найти псевдообратную к ней, мы точно значем, что псевдообратная матрица будет иметь размерность $(2, 1)$.
    Т.е псевдобратная матрица это некий вектор \(\left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right)\).
    Запишем, все аксиомы:

    \begin{itemize}
        \item $(1 \ 1) \left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) (1 \ 1) = (1 \ 1)$
        \item $\left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) (1 \ 1) \left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) = \left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) $
        \item $((1 \ 1) \left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right))^{*} =  (1 \ 1) \left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right)$
        \item $(\left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) (1 \ 1))^{*} = \left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) (1 \ 1)$
    \end{itemize}

    Заметим, что третий пункт всегда выполняется, значит его можно исключить, выполним перемножение матриц

    \begin{itemize}
        \item $(a + b) (1 \ 1) = (1 \ 1)$
        \item $\left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) (a + b) = \left(\begin{smallmatrix} a \\ b \end{smallmatrix}\right) $
        \item $ \left(\begin{smallmatrix} a & a \\ b & b \end{smallmatrix}\right)^{*} = \left(\begin{smallmatrix} a & a \\ b & b \end{smallmatrix}\right)$
    \end{itemize}

    Следовательно $a = b$ и $a + b = 1$ таким образом псевдообратная матрица имеет вид \(\left(\begin{smallmatrix} 0.5 \\ 0.5 \end{smallmatrix}\right)\)

    \subsubsection{Общий случай}

    В общем случае для вектора $\bar{a}$ псевдообратная матрица будет иметь вид:

    \begin{equation}
        \bar{a}^{+} = \frac{\bar{a}^{*}}{\Vert a \Vert^{2}}
    \end{equation}

    Покажем, что все аксиомы выполняются

    \begin{itemize}
        \item $  \bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}} \bar{a}   =  \bar{a}$
        \item $\frac{\bar{a}^{*}}{\Vert a \Vert^{2}} \bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}} = \frac{\bar{a}^{*}}{\Vert a \Vert^{2}}$
        \item $(\bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}})^{*} = \bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}}$
        \item $( \frac{\bar{a}^{*}}{\Vert a \Vert^{2}} \bar{a})^{*} = \frac{\bar{a}^{*}}{\Vert a \Vert^{2}} \bar{a}$
    \end{itemize}

    Это равносильно
    (Note: для расчета нормы комплексного вектора используется не транспонирование, а комплексное сопряжение)

    \begin{itemize}
        \item $  \bar{a} \cdot 1 =  \bar{a}$
        \item $1 \cdot \frac{\bar{a}^{*}}{\Vert a \Vert^{2}} = \frac{\bar{a}^{*}}{\Vert a \Vert^{2}}$
        \item $(\bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}})^{*} = \bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}}$
        \item $1^{*} = 1$
    \end{itemize}

    Таким образом нам нужно, чтобы выполнялось выражение

    \begin{equation}
        (\bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}})^{*} = \bar{a} \frac{\bar{a}^{*}}{\Vert a \Vert^{2}}
    \end{equation}

    Сократим константу

    \begin{equation}
        (\bar{a} \bar{a}^{*})^{*} = \bar{a} \bar{a}^{*}
    \end{equation}

    Раскроем эрмитово сопряжение

    \begin{equation}
        \bar{a} \bar{a}^{*} = \bar{a} \bar{a}^{*}
    \end{equation}

    Следовательно, аксиома 3 тоже выполняется.
    А по условию единственности это и есть псевдообратная матрица, легко показать, что для вектора строки ситуация будет аналогичной

    \subsection{Лемма об обратимости $A^{*} A$}

    Пусть $A$ --- матрица полного столбцового ранга, то матрица $A^{*} A$ --- невырожденная квадрантная матрица.
    (Для матрицы полного строкового ранга все практически 1 в 1 сейм)

    \quad

    \textbf{Доказательство}

    Пусть $A$ --- матрица $(m, n)$, такая, что $n < m$ и $\operatorname{rang}(A) = n$.
    Заметим, что $\operatorname{rang}(A^{*} A) <= \operatorname{rang}(A)$ (по свойству произведения матриц).
    Тогда для док-ва нам достаточно показать, что $\operatorname{rang}(A^{*} A) >= \operatorname{rang}(A)$.
    По свойству ядра, если $X$ имеет размерность $(m, n)$, то $\dim(\ker(X)) + \operatorname{rang}(X) = n$.
    Пусть $x \in \ker(A^{*} A)$, тогда $A^{*} A x = 0$, домножим на $x^{*}$ справа, тогда выражение примет вид
    $(Ax)^{*} Ax = 0$.
    Это скалярное произведение вектора $Ax$ на самого себя и по свойству скалярного произведения мы получаем, что
    $Ax = 0$, при этом $\dim(\ker(A)) = 0$, следовательно $\dim(\ker(A^{*} A)) = 0$, а тогда $\operatorname{rang}(A^{*} A) = n$

    В общем случае мы доказали, что $\ker(A^{*} A) \subset \ker(A) $, отсюда вытекает неравенство
    $\dim(\ker(A^{*} A)) <= \dim(\ker(A))$, то равносильно $n - \operatorname{rang}(A^{*} A) <= n - \operatorname{rang}(A)$, что приводит нас к
    $\operatorname{rang}(A^{*} A) >= \operatorname{rang}(A)$
    
    \quad
    
    \subsection{Теорема о существовании псевдообратной матрицы для матриц полного столб. и строч. ранга}

    Если $A$ матрица $(m, n)$, такая что $n < m$ и что $\operatorname{rang}(A) = n$ (матрица полного столбцового ранга), то $A^{+}$ существует
    и равна

    \begin{equation}
        A^{+} = (A^{*} A)^{-1} A ^{*}
    \end{equation}

    Для матрицы полного строчного ранга

    \begin{equation}
        A^{+} = A^{*} (A A ^{*})^{-1}
    \end{equation}

    Проверим аксиомы для матрицы полного столбцового ранга

    \begin{itemize}
        \item $A (A^{*} A)^{-1} A ^{*} A = A$
        \item $(A^{*} A)^{-1} A ^{*} A (A^{*} A)^{-1} A ^{*} = (A^{*} A)^{-1} A ^{*}$
        \item $(A (A^{*} A)^{-1} A ^{*})^{*} = A (A^{*} A)^{-1} A ^{*}$
        \item $((A^{*} A)^{-1} A ^{*}A)^{*} = (A^{*} A)^{-1} A ^{*}A$
    \end{itemize}

    Это сводится к
    (3 точно выполняется)

    \begin{itemize}
        \item $A E = A$
        \item $(A^{*} A)^{-1} E A ^{*} = (A^{*} A)^{-1} A ^{*}$
        \item $(E)^{*} = E$
    \end{itemize}

    Следовательно, данная матрица удовлетворяет всем четырем аксиомам и единственна

    \textbf{Аналогично, для матриц полного строчного ранга}


    \subsection{Теорема о существовании псевдообратной матрицы для любой матрицы}

    \subsubsection{Лемма об обратной матрице произвдения матриц}

    Если $A = BC$, где $B$ --- матрица полного столбцового ранга,
    а $C$ --- матрица полного строкового ранга, то тогда $A^{+} = B^{+} C^{+}$,

    \quad

    \textbf{Доказательство}

    \quad

    Рассмотрим, аксиомы для $A$, пусть псевдообратнная матрица $A^{+} = C^{+} B^{+} $, если для нее будут выполнены аксиомы,
    то тогда по условию единственности, это и есть псевдообртаная матрица

    \begin{itemize}
        \item $A C^{+} B^{+} A = A$
        \item $C^{+} B^{+} A C^{+} B^{+} = C^{+} B^{+}$
        \item $(A C^{+} B^{+})^{*} = AC^{+} B^{+}$
        \item $(C^{+} B^{+} A)^{*} = C^{+} B^{+}A$
    \end{itemize}

    Заменим $A$ на $BC$, тогда

    \begin{itemize}
        \item $BC C^{+} B^{+} BC = BC$
        \item $C^{+} B^{+} BC C^{+} B^{+} = C^{+} B^{+}$
        \item $(BC C^{+} B^{+})^{*} = BCC^{+} B^{+}$
        \item $(C^{+} B^{+} BC)^{*} = C^{+} B^{+}BC$
    \end{itemize}

    Так как мы знаем, что за матрицы $B$ и $C$ мы можем выразить их псведообратные матрицы явно

    \begin{itemize}
        \item $BC ( C^{*} (C C ^{*})^{-1} ) ( (B^{*} B)^{-1} B^{*} ) BC = BC$
        \item $(C^{*} (C C ^{*})^{-1}) ( (B^{*} B)^{-1} B^{*} ) BC (C^{*} (C C ^{*})^{-1}) ( (B^{*} B)^{-1} B^{*}) = (C^{*} (C C ^{*})^{-1})( (B^{*} B)^{-1} B^{*})$
        \item $(BC (C^{*} (C C ^{*})^{-1}) ((B^{*} B)^{-1} B^{*}))^{*} = BC (C^{*} (C C ^{*})^{-1}) ((B^{*} B)^{-1} B^{*})$
        \item $((C^{*} (C C ^{*})^{-1}) ( (B^{*} B)^{-1} B^{*}) BC)^{*} = (C^{*} (C C ^{*})^{-1}) ( (B^{*} B)^{-1} B^{*}) BC$
    \end{itemize}

    Начнем раскрывать скобки

    \begin{itemize}
        \item $B C = BC$
        \item $(C^{*} (C C ^{*})^{-1})  ( (B^{*} B)^{-1} B^{*}) = (C^{*} (C C ^{*})^{-1})( (B^{*} B)^{-1} B^{*})$
        \item $(B ((B^{*} B)^{-1} B^{*}))^{*} = B (B^{*} B)^{-1} B^{*}$
        \item $((C^{*} (C C ^{*})^{-1}) C)^{*} = (C^{*} (C C ^{*})^{-1}) C$
    \end{itemize}

    Легко заметить, что все 4 аксиомы выполняются

    \subsubsection{Основная теорема}

    Для любой матрицы существует псевдообратная матрица

    \quad

    \subsubsection{Начало доказательства}

    \quad

    Мы знаем, что такая матрица есть для невырожденных матриц,
    матриц полного столбцового и строкового рангов и нулевых матриц,
    рассмотрим ситуацию для других матриц, которые не попадают в эту категорию.
    Мы знаем, что для любой матрицы можно применить скелетное разложение, так и сделаем для произвольной матрицы
    $A$, тогда $A = FG$, по свойству леммы о произведении матриц мы знаем, что $A^{+} = G^{+} F^{+}$.
    При этом, мы знаем, что для любой матрицы полного столбцового ранга и полного строчного ранга, существует псвдообратная матрица,
    следовательно $A^{+} = G^{*} (G G ^{*})^{-1} (F^{*} F)^{-1} F ^{*}$.
    Следовательно, для любой матрицы существует псевдообратная.
    
    \subsection{Суть псевдообратной матрицы}

    В случае несовместных систем псевдообратная матрица позволяет получить наиболее точное приближенное решение по МНК.
    Сформулируем задачу более точно.
    Вектор $\bar{u}$ называется приближенным решением системы $A \bar{x} = \bar{b}$ по МНК или псевдорешением, если для любого $\bar{x}$ выполняется

    \begin{equation}
        \Vert A \bar{x} - \bar{b} \Vert \geq \Vert A \bar{u} - \bar{b} \Vert
    \end{equation}

    Используется Евклидова норма, легко заметить, что если система имеет решение, то оно совпадает с псевдорешением
    
    \subsection{Теорема о псевдорешении}

    Вектор $\bar{u} = A^{+} \bar{b}$ является псевдорешением и 
    имеет наименьшую длину из всех псевдорешений 

    \quad

    \textbf{Доказательство}

    \quad

    Заметим, что $\operatorname{Im}(A)$ порождено столбцами матрицы $A$, а все векторы $\ker(A)$ ортогональны строкам матрицы $A$.
    Следовательно для любых $\bar{x} \in \operatorname{Im}(A)$ и $\bar{y} \in \ker(A^{*})$ выполняется $\bar{x} \cdot \bar{y} = 0$.
    Аналогично и в обратную сторону для $\bar{x} \in \operatorname{Im}(A^{*})$ и $\bar{y} \in \ker(A)$

    \subsubsection{Лемма}

    Для любого $\bar{x} \in \operatorname{Im}(M)$ и $\bar{y} \in \operatorname{Im}(A)$,
    выполняется $\bar{x} \cdot \bar{y} = 0$, если $M = A A^{+} - E$


    \quad

    \textbf{Доказательство}

    \quad

    Если мы покажем, что $\operatorname{Im}(M) \subset \ker(A^{*})$,
    тогда точно лемма будет доказана по свойству выше.
    Рассмотрим вектор $y = Mx \in \operatorname{Im}(M)$, покажем, что $A^{*}y = 0$.
    $A^{*}Mx = 0$, следовательно $(A^{*} A A^{+} - A^{*}) x = 0$.
    По аксиоме три $(A^{*} (A A^{+})^{*} - A^{*}) x = 0$.
    Это равняется $((A A^{+} A)^{*} - A^{*}) x = 0$.
    Что приводится к $( A^{*} - A^{*}) x = 0$.
    Таким образом данное равенство выполняется всегда. \textbf{Ч.Т.Д}


    \subsubsection{Возврат к доказательству}

    Если $\bar{x} \cdot \bar{y} = 0$, то тогда для $\bar{z} = \bar{x} + \bar{y}$
    выполняется следующее равенство $\Vert \bar{z} \Vert^{2} = \Vert \bar{x} \Vert^{2} + \Vert \bar{y} \Vert^{2}$.
    Покажем это $\Vert \bar{z} \Vert = \sqrt{\bar{z} \cdot \bar{z}}$, раскрывая это получим:
    $\sqrt{(\bar{x} + \bar{y}) \cdot (\bar{x} + \bar{y})}$, что приводит нас к выражению $\sqrt{ \bar{x} \cdot \bar{x} + \bar{y} \cdot \bar{y} }$.
    Следовательно мы получаем, что $\Vert \bar{z} \Vert^{2} = \Vert \bar{x} \Vert^{2} + \Vert \bar{y} \Vert^{2}$. Ч.Т.Д

    Отсюда мы получаем неравенство: $\Vert \bar{z} \Vert \geq \Vert \bar{x} \Vert$ (или $\bar{y}$)

    \quad 

    Покажем, что $u = A^{+} b$ является псевдорешением. Рассмотрим 
    $f(x) = Ax - b$ нам нужно показать, что такой вектор имеет длину больше 
    чем $AA^{+}b - b$. Мы можем заметить, что второе выражние равно 
    $(AA^{+} - E)b$. A $Ax \in \operatorname{Im}(A)$, следовательно 
    эти два вектора перпендикулярны.

    Мы можем записать $f(x)$ как $Ax - Au + Au - b$, 
    где $u = A^{+} b$

    \begin{equation}
        Ax - Au + Au - b = A(x - u) + (AA^{+} - E) b
    \end{equation}

    Легко увидеть, что данные слагаемые перпендикулярны, таким 
    образом исходя из неравенства выше 

    \begin{equation}
       \Vert A(x - u) + (AA^{+} - E) b \Vert \geq \Vert (AA^{+} - E) b \Vert
    \end{equation}

    Раскрывая скобки получим, что для любого $x$ 

    \begin{equation}
        \Vert f(x) \Vert \geq \Vert AA^{+}b - b \Vert
    \end{equation}

    Легко заметить, что равенство достигается при 

    \begin{equation}
        A(x - A^{+}b ) = 0
    \end{equation}

    \quad 

    \textbf{Покажем, что такое решение имеет минимальную длину по сравнению с другими 
    псевдорешениями} 

    \quad 

    Пусть $x$ другое псевдорешение, т.е $A(x - u) = 0$

    Покажем, что $ (x - u) \perp u$, при условии $A(x - u) = 0$.
    Рассмотрим их произвдение

    \begin{equation}
        (x - u)^{*} \cdot u = b^{*} {A^{+}}^{*} (x - u)
    \end{equation}

    При этом ${A^{+}}^{*} = (A^{+} A A^{+})^{*} = {A^{+}}^{*} {A^{+} A}^{*}$. 
    Итого мы получаем, что скалярное произвдение имеет вид 

    \begin{equation}
        b^{*} {A^{+}}^{*} A^{+} A (x - u) = 0
    \end{equation}

    \text{Ч.Т.Д}

    Используя прошлое неравенство мы получим, что 

    \begin{equation}
        \Vert x - u + u \Vert \geq \Vert u \Vert 
    \end{equation}

    Мы доказали, что псевдорешение $A^{+} b$ имеет минимальную длину 
    среди всех возможных 

    \subsubsection{Как описать все псевдорешения?}

    Все псевдорешения могут быть описаны формулой 
    
    \begin{equation}
        A (x - A^{+}b) = 0 
    \end{equation}
    
    Можно заметить, что, если 

    \begin{equation}
        x = A^{+}b - (A^{+}A - E) y
    \end{equation}
    где $y$ --- это произвольный вектор, то тогда подставив такой $x$ в формулу 
    мы получим, что 

    \begin{equation}
        A (A^{+}b - (A^{+}A - E) y - A^{+}b) = - A (A^{+}A - E) y = - (A - A) y = 0
    \end{equation}

    Следовательно любой такой вектор подходит в качестве псевдорешения, т.е минимизирует норму между 
    собой и целевым вектором

    \section{Матричные разложения}

    \subsection{LU разложение}

    $LU$ разложение позволяет представить матрицу как произведение 
    двух матриц $L$ --- нижнетреугольная матрица с единицами на диагонали, а $U$ - верхнетреугольная 

    \subsubsection{Теорема об $LU$ разложении}

    Пусть $A$ --- матрица $(n, n)$ и все угловые подматрицы невырождены.
    Тогда существует единсвтенное разложение $A = LU$, где $L$ --- нижнетреугольная 
    с единицами на диагонали, а $U$ --- верхнетреугольная матрица 

    \quad 

    \textbf{Доказательство}

    \quad 

    Докажем по индукции, что такое разложение существует, база индукции выглядит так: для матрицы $(1, 1)$ мы получим
    $L = \left(\begin{smallmatrix} 1 \end{smallmatrix}\right)$ и $U = \left(\begin{smallmatrix} a_{11} \end{smallmatrix}\right)$

    \quad 

    Пусть разложение существует для любой матрицы размерами $(n - 1, n - 1)$ для которой выполнены требования, 
    покажем что в таком случае оно существует для любой матрицы $A$ с размерами $(n, n)$. Представим $A$ в блочном виде

    \begin{equation}
        A = \begin{bmatrix}
            A_{11} & a_{12} \\ a_{21} & a_{nn}
        \end{bmatrix}
    \end{equation}

    Тут матрица $A_{11}$ имеет размеры $(n - 1, n - 1)$ по условию все угловые миноры ненулевые, следовательно для данной подматрицы сущесвтвует 
    $LU$ разложение $A_{11} = L_{11}U_{11}$

    Мы хотим представить $A$ как $LU$, рассмотрим как должны 
    выглядеть такие матрицы в предположении, что $L$ и $U$ будут занимать соответсвующие им блоки  

    \begin{equation}
        L = \begin{bmatrix}
            L_{11} & 0 \\
            l_{21} & 1
        \end{bmatrix}
    \end{equation}

    и 

    \begin{equation}
        U = \begin{bmatrix}
            U_{11} & u_{12} \\ 
            0 & u_{nn}
        \end{bmatrix}
    \end{equation}

    Тогда произведение матриц задается как 

    \begin{equation}
        LU = \begin{bmatrix}
            L_{11} U_{11} & L_{11} u_{12} \\ 
            l_{21} U_{11} & l_{21} u_{12} + u_{nn}
        \end{bmatrix}
    \end{equation}

    Нужно заметить, что $l_{12}$ --- вектор строка

    \quad

    Запишем наши равенства 

    \[
    \begin{cases}
    L_{11} u_{12} = a_{12} \\
    l_{21} U_{11} = a_{21} \\
    l_{21} u_{12} + u_{nn} = a_{nn}
    \end{cases}
    \]
    Тут $l_{12}$ --- вектор строка и $u_{12}$ --- вектор столбец

    \quad

    $L_{11}$ однозначно обратима, а $U_{11}$ так как все угловые миноры неотрицатльны ($\det(A_{11}) = \det(L_{11}) \cdot \det(U_{11})$
    и при этом $\det(A_{11}) \neq 0$)

    \[
    \begin{cases}
    u_{12} = L_{11}^{-1} a_{12} \\
    l_{21} = a_{21} U_{11}^{-1} \\
    u_{nn} = a_{nn} - a_{21} U_{11}^{-1}L_{11}^{-1} a_{12} = a_{nn} - a_{21} A_{11}^{-1} a_{12} 
    \end{cases}
    \]

    Мы можем заметить, что такое разложение единственно, и для него строго необходимо, чтобы матрица в левом верхнем блоке обращалась

    \subsubsection{Практический подход для LU разложения}

    Мы можем привести матрицу $A$ к нижнетреугольному виду с помощью метода Гаусса, а после найти $U$ как $U = L^{-1}A$


    \subsubsection{Где используется}

    Можно выделить следующие use-case: 

    \begin{itemize}
        \item Быстрое решение систем вида $Ax = y$. Такая система превращается в две треугольные системы при ее представлении как: $LUx = y$    
        \item Численно стабильное решение задачи нахождения $\det$, так как мы можем использовать натуральный логарифм от 
        детерминанта и при этом сохранять численную стабильность решения, так как $\det(A) = \det(L) \cdot \det(U) = 1 \cdot \prod_{i}^{n} U_{ii}$  
        \item Более удобное обращение раскладываемой матрицы
    \end{itemize}

    \subsection{LUP-разложение}

    Данное разложение является обобщением $LU$ разложения на случай любой квадратной невырожденной матрицы. Любая матрица может быть представлена в виде: 

    \begin{equation}
        PA = LU
    \end{equation}

    Т.е мы выполняем разложение для матрицы $PA$, где $P$ отвчеает за перестановку строк в матрице $A$. Мы всегда можем привести невырожденную матрицу $A$ к такому виду, чтобы 
    к ней можно было применить $LU$ разложение, это следует из метода окаймляющих миноров для определения ранга.

    Хорошим алгоритмом для $LUP$ разложения является алгоритм Дулиттла (рассмотрим позже), который позоляет удобно векторизовать вычисления. 
    Однако алгоритмическая сложность все также составляет $O(n^{3})$


    \subsection{$LDL^{T}$ разложение}

    В случае если матрица $A$ является эрмитивой ($A^{*} = A$) и положительно определенной, то тогда $LU$ разложение будет связанно с
    $LDL^{T}$ разложением. Покажем это: 

    \quad 

    Так как матрица $A$ положительно определена все угловые миноры положительны (по критерию Сильвестра), 
    следовательно существует $LU$ разложение. Также по условию эрмитовости мы получаем: 

    \begin{equation}
        A = LU = A^{*} = U^{*} L^{*}
    \end{equation}
    
    Т.е мы получаем равенство которое нам нужно решить 

    \begin{equation}
        LU = U^{*} L^{*}
    \end{equation}

    Заметим, что всегда существует такая матрица $D$, что 

    \begin{equation}
        U = D L^{*}
    \end{equation}
    так как $L^{*}$ всегда обратима и мы можем выразить $D$ как $U {L^{*}}^{-1}$. $U$ должна быть верхнетреугольной, следовательно
    матрица $D$ будет диагонаальной, так как она не должна менять структуру матрицы $L^{*}$, которая сама является верхнетреугольной.

    \quad 

    Подставим $U = D L^{*}$ в исходное уравнение. Тогда: 

    \begin{equation}
        L D L^{*} =  L D^{*} L^{*}
    \end{equation}

    Так как $D^{*} = D$ равенство выполняется ($D$  --- диагональная)

    \quad 

    Таким образом, мы показали, что любую эрмитову положительно опредленную матрицу можно представить в виде следующего произвдения матриц, причем такое 
    разложение единственно

    \begin{equation}
        A = LDL^{*}
    \end{equation}
    где $L$ --- нижнетреугольная матрица, а $D$ --- диагональная 

    \quad 

    Заметим интересный факт: у $D$ на диагонали стоят только положительные числа, так как матрица $A$ положительно определена. 
    Проверим этот факт: мы знаем, что $x^{*} A x > 0$ для любого $x$. Т.е $x^{*} LDL^{*} x > 0$. 
    Выразим $y = L^{*}x$, при этом $L^{*}$ --- невырожденная. Тогда выражение равносильно $y^{*}D y > 0$ для любого $y$.
    Так как матрица $D$ диагональная: 

    \begin{equation}
        y^{*}D y = \sum_{i=1}^{n}{d_{ii} \cdot y_{i}^{2}}
    \end{equation}
    где $d_{ii}$ - элемент матрицы $D$

    Следовательно все $d_{ii} > 0$, иначе найдется вектор для которого условие будет невыполнено 


    \subsection{Разложение Холецкого}

    Пусть нам дана эрмитова квадратная положительно определенная матрица, тогда она представима в виде: 

    \begin{equation}
        A = U^{*}U = L L^{*}
    \end{equation}

    \quad 
    
    При этом на главной диагонали $L$ или $U$ будут находиться вещесвтенные положительные числа.

    \quad 

    Заметим, что мы можем представить нашу матрицу так (прошлое разложение): 

    \begin{equation}
        A = L D L^{*}
    \end{equation}

    Так как матрица $D$ --- диагональная и имеет только положительные элементы на диагонали, 
    мы можем определить диагональную матрицу $D^{0.5}$, такую что $D^{0.5} D^{0.5} = D$.

    \quad 

    Следовательно: 

    \begin{equation}
        A = L D^{0.5} {D^{0.5}}^{*} L^{*}
    \end{equation}

    Заметим, что тогда

    \begin{equation}
        A = L_{1} L_{1}^{*}
    \end{equation}

    Где $L_{1} = L D^{0.5}$, при этом $D^{0.5}$ не меняет структуру матрицы $L$, так как является диагональной матрицей. 
    Также стоит отметить, что на диагонали у $D^{0.5}$ находятся положительные числа, а именно квадраты значений с диагонали $D$. 
    Таким образом на диагонали матрицы $L_{1}$ будут стоять только положительные числа, что и требовалось.
    Следовательно мы показали, что можно единсвтенным образом представить матрицу $A$ как: 

    \begin{equation}
        A = L L^{*}
    \end{equation}

    На практике лучше считать $LDL^{T}$ разложение, так как оно не включает в себя взятие 
    квадрантных корней

    \subsection{QR - разложение}

    $QR$ разложение представляет собой создание оронормированного базиса в матричном виде.
    Вспомним про алгоритм ортоганализации: мы берем первый вектор как есть. 
    Теперь нам нужно сделать так, чтобы второй вектор стал перпендикулярен первому.
    Мы можем прдставить вектор $a_{2}$ как сумму двух компонент: перпендикуляра к $a_{1}$ и 
    некого остатка. Т.е 

    \begin{equation}
        a_{2} = b_{2} + rem_{2}
    \end{equation}
    При этом $b_{2} \perp a_{1}$
    
    \quad

    Мы можем найти остаток как $\frac{\langle a_{2}, a_{1} \rangle}{\langle a_{1}, a_{1} \rangle} a_{1}$

    \quad 

    Можно считать, что на каждом этапе мы строим плоскость из векторов которые нашли до этого. 
    При этом новый вектор не прнадлежит этой плоскости так как 
    тогда не выполнялось бы условие о ЛНЗ. Мы всегда можем провести от плоскости перпендикуляр к некой точке, тогда наш вектор вне этой плоскости 
    представим как сумма перпендикуляра от плоскости и некого вектора который находится на плоскости и ведет к точке где начинается перпендикуляр.
    Благодаря данному механизму любой новый вектор перпендикулярен всем предыдущим и все будущие вектора будут
    перпендикулярны текущему, так как он будет использоваться для создания плоскости. После того как все вектора будут найдены 
    их будет необходимо отнормировать.
    
    \quad 

    \textbf{Что такое QR разложение?}
    
    \quad 

    Это представление матрицы $A$ в виде 

    \begin{equation}
        A = QR 
    \end{equation}
    где в столбцах матрицы $Q$ стоят вектора из ортонормированного базиса (i.e $Q^{*}Q = E$). 
    А $R$ --- верхнетреугольная матрица. 

    \quad 

    Самым простым способом нахождения разложения является нахождение $Q$ через ортоганализацю Грамма Шмидта, и последующее нахождение 
    $R$ как $R = Q^{-1}A = Q^{*}A$

    \quad 

    Однако эффективнее использовать алгоритм Хаусхолдера, для матрицы $(m, n)$, где 
    $m > n$. Его алгоритмическая сложность составляет $O(2mn^{2} - \frac{2 n^{3}}{3})$.

    \subsection{Спектральное разложение}

    Некоторые квадратные матрицы можно представить в виде: 

    \begin{equation}
        A = U D U^{*}
    \end{equation}
    где $D$ --- диагональная матрица с собственными числами на диагонали, а $U$ --- унитарная матрица (i.e $UU^{*} = U^{*}U = E$) 
    (вообще то она ортонормированна по столбцам, но в случае квадратных матриц это приводит и к ортонормированности по строкам)

    Для этого нужно затребовать, чтобы матрица была 

    \begin{itemize}
        \item Эрмитовой (все ее собственные числа будут вещесвтенными)
        \item Унитарной (все ее собственные числа будут по модулю равны 1)
        \item Нормальной ($A^{*}A = A A^{*}$)
    \end{itemize}

    Для его нахождения мы сначала должны найти все собственные числа матрицы $A$, после найти весь набор 
    собственных векторов $A$. Мы можем привести наши собственные вектора к ортонормированному базису и записать 
    эти вектора в столбцы матрицы $U$. А в $D$ мы запишем собтсвенные числа матрицы $A$.

    \quad 

    Покажем, что нормальность необходима для данного разложения, пусть матрица не является нормальной, тогда 
    $A^{*}A \neq A A^{*}$. Подставим сюда матрицу в разложенном виде 

    \begin{equation}
        U D U^{*} U D U^{*} \neq U D U^{*} U D U^{*}
    \end{equation}

    Сокращая часть множителей

    \begin{equation}
        U D D U^{*} \neq U D D U^{*}
    \end{equation}

    Как мы видим возникает противоречие.

    Таже из нормальности следует, что матрица имеет все собственные вектора (лень доказывать)


    \subsection{Сингулярное разложение (SVD)}

    Данное разложение является обобщением спектрального разложения на случай произвольной матрицы.
    Пусть $A$ матрица $(m, n)$, тогда собственные числа матрицы $A{*}A$, которая является Эрмитовой, будут вещественными числами.
    Таким образом мы можем взять корень из этих чисел.

    \begin{equation}
        \sigma(A)_{i} = \sqrt{ \lambda(A^{*}A)_{i} }
    \end{equation}
    где $\sigma(A)_{i}$ --- называется сингулярным числом матрицы $A$ 

    \quad 

    Есть договоренность в нумерации сингулярных чисел, что 

    \begin{equation}
        \sigma_{1} \geq \sigma_{2} ... \geq \sigma_{n}
    \end{equation}

    Легко заметить, что $\operatorname{rang}(A^{*}A) = \operatorname{rang}(A)$, следовательно
    кол-во сингулярных значений равно рангу матрицы $A$

    \quad 

    \textbf{Теорема}

    \quad 

    Для любой матрицы $A$ размерами $(m, n)$ сущесвтвует сингулярное разложение, такое, что 

    \begin{equation}
        A = U \Sigma V^{*}
    \end{equation}
    где: 

    \begin{itemize}
        \item $U$ --- унитарная матрица $(m, m)$
        \item $\Sigma$ ---диагональная матрица с размерами совпадающими с матрицей $A$ на 
        на диагонали у которой стоят сингулярные значения 
        \item $V$ --- унитарная матрица $(n, n)$
    \end{itemize}

    \subsubsection{Отступление}


    Также стоит отметить, что есть вторая версия разложения --- усеченное разложение

    Матрица $A$ будет представлена как 

    \begin{equation}
        A = U_{(m, r)} \Sigma_{(r, r)} V^{*}_{(r, n)}
    \end{equation}

    Мы просто вырезаем все строки и столбцы, которые уможаются на ноль


    \subsubsection{Intuition данного разложения}


    Рассмотрим матрицу $A$ размерами $(m, n)$. Это отображение $f:R^{n} \rightarrow R^{m}$. 
    Таким образом, данное разложение говорит нам, что в исходном и выходном пространстве найдутся такие ортонормированные базисы, что действие оператора 
    будет описываться диагональной матрицей.

    \quad 

    Пусть у нас есть базис $v_{1}, v_{2} ... v_{n}$ в исходном пространстве и $u_{1}, u_{2} ... u_{m}$ в выходном тогда: 

    \begin{equation}
        f(\sum_{i = 1}^{n}{x_{i} \cdot v_{i}}) = \sum_{i = 1}^{m}{x_{i} \cdot \sigma_{i} \cdot u_{i}}
    \end{equation}

    \subsubsection{Связь с нормой Фробениуса}

    Во-первых покажем, что норма Фробениуса инвариантна относительно униитарных преобразований слева 
    (Под унитарным преобразованием мы можем подразуемвать, что столбцы матрицы задают ортонормированный базис (но не обязательно строки) т.е $U^{*}U = E$). 
    Т.е нам нужно показать, что  

    \begin{equation}
        \Vert U  A  \Vert_{fro} = \Vert A \Vert_{fro} 
    \end{equation}
    где $U$ --- матрица с ортонормированными столбцами ($U^{*}U = E$)

    \quad 

    Легко инуитивно заметить, что оронормированный базис не должен менять норму вектора так как мы просто поворачиваем весь базис относительно исходного $e_{1}, e_{2} ... e_{n}$.

    \quad 

    Также легко заметить, что 

    \begin{equation}
        \Vert B \Vert_{fro} = \sqrt{ \sum_{i = 1}^{k} \Vert B_{(*, i)} \Vert_{L2}^{2} }
    \end{equation}
    Т.е норма Фробениуса выражается через квадраты L2 норм столбцов
    
    \quad 

    Нам достаточно показать, что переход к новому ортонормированному базису не меняет норму вектора столбца и тогда утверждение будет доказано.
    Рассмотрим выражние $U x$. Нам необходимо показать, что $\Vert Ux \Vert_{L2} = \Vert x \Vert_{L2}$. Заметим, что 

    \begin{equation}
        \Vert x \Vert_{L2} = \sqrt{\langle x, x \rangle} = \sqrt{x^{*} \cdot x}
    \end{equation}

    при этом 

    \begin{equation}
        \Vert Ux \Vert_{L2} = \sqrt{\langle  Ux,  Ux \rangle} = \sqrt{x^{*} U^{*} \cdot U x} = \sqrt{x^{*} \cdot x}
    \end{equation}

    Таким образом: 

    \begin{equation}
        \Vert A \Vert_{fro} = \sqrt{ \sum_{i = 1}^{k} \Vert A_{(*, i)} \Vert_{L2}^{2} }
    \end{equation}

    Для $UA$

    \begin{equation}
        \Vert UA \Vert_{fro} = \sqrt{ \sum_{i = 1}^{k} \Vert U \cdot A_{(*, i)} \Vert_{L2}^{2} }
    \end{equation}

    Исходя из предыдущего выражения мы знаем, что 

    \begin{equation}
        \Vert U \cdot A_{(*, i)} \Vert_{L2} =  \Vert A_{(*, i)} \Vert_{L2}
    \end{equation}

    В итоге 

    
    \begin{equation}
        \Vert UA \Vert_{fro} = \sqrt{ \sum_{i = 1}^{k} \Vert U \cdot A_{(*, i)} \Vert_{L2}^{2} } = \sqrt{ \sum_{i = 1}^{k} \Vert A_{(*, i)} \Vert_{L2}^{2} } = \Vert A \Vert_{fro}
    \end{equation}

    Рассмотрим норму матрицы $A$

    \begin{equation}
        A = U \Sigma V^{*}
    \end{equation}

    Тогда 

    \[
    \begin{gathered}
        \Vert A \Vert_{fro} = \Vert U \Sigma V^{*} \Vert_{fro} = \Vert \Sigma V^{*} \Vert_{fro} \\ 
        = \Vert V \Sigma^{*} \Vert_{fro} = \Vert \Sigma^{*} \Vert_{fro} = \Vert \Sigma \Vert_{fro} \\ 
        = \sum \sigma(A)^{2}
    \end{gathered}
    \]
    


    

\end{document}